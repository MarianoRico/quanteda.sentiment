% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textstat_valence.R
\name{textstat_valence}
\alias{textstat_valence}
\title{Compute sentiment from word valences}
\usage{
textstat_valence(
  x,
  dictionary,
  normalization = c("dictionary", "all", "none"),
  ...
)
}
\arguments{
\item{x}{a character, \link[quanteda:corpus]{quanteda::corpus}, \link[quanteda:tokens]{quanteda::tokens}, or \link[quanteda:dfm]{quanteda::dfm} object containing
text, tokens, or features whose sentiment will be scored.}

\item{dictionary}{a \pkg{quanteda} \link[quanteda:dictionary]{quanteda::dictionary} that has \link{valence} set, in
the form of numerical valences associated with sentiment}

\item{normalization}{the baseline for normalizing the sentiment counts after
scoring. Sentiment scores within keys are weighted means of the tokens
matched to dictionary values, weighted by their valences.  The default
\code{"dictionary"} is to average over only the valenced words.  \code{"all"}
averages across all tokens, and \code{"none"} does no normalization.}

\item{...}{not used here}
}
\value{
a data.frame of sentiment scores
}
\description{
Compute sentiment scores from tokens or document-feature matrices, based on
the valences of dictionary keys and values.
}
\note{
If the input item is a \link[quanteda:dfm]{quanteda::dfm}, then multi-word values will not be matched
unless the features of the \link[quanteda:dfm]{quanteda::dfm} have been compounded previously.  The input
objects should not have had dictionaries applied previously.
}
\examples{
library("quanteda")
\dontrun{

# AFINN
afinn <- read.delim(system.file("extdata/afinn/AFINN-111.txt", 
                                package = "quanteda.sentiment"),
                    header = FALSE, col.names = c("word", "valence"))
data_dictionary_afinn <- dictionary(list(afinn = afinn$word))
valence(data_dictionary_afinn) <- list(afinn = afinn$valence)
textstat_valence(toks, dictionary = data_dictionary_afinn)

# ANEW
anew <- read.delim(url("https://bit.ly/2zZ44w0"))
anew <- anew[!duplicated(anew$Word), ] # because some words repeat
data_dictionary_anew <- dictionary(list(pleasure = anew$Word,
                                        arousal = anew$Word,
                                        dominance = anew$Word))
valence(data_dictionary_anew) <- list(pleasure = anew$ValMn,
                                      arousal = anew$AroMn,
                                      dominance = anew$DomMn)
textstat_valence(toks, data_dictionary_anew["pleasure"])
textstat_valence(toks, data_dictionary_anew["arousal"])}

}
\references{
For a discussion of how to aggregate sentiment scores to the document
level, see:

Lowe, W., Benoit, K. R., Mikhaylov, S., & Laver, M. (2011).
Scaling Policy Preferences from Coded Political Texts. \emph{Legislative Studies
Quarterly}, 36(1), 123â€“155.
\doi{10.1111/j.1939-9162.2010.00006.x}
}
\seealso{
\code{\link[=valence]{valence()}}
}
